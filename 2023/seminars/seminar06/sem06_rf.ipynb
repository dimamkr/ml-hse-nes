{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 6: Случайный лес\n",
    "[источник](https://github.com/hse-ds/iad-intro-ds/tree/master/2023/seminars/sem13_rf)\n",
    "## 1. Bias-Variance decomposition\n",
    "\n",
    "Вспомним, что функцию потерь в задачах регрессии или классификации можно разложить на три компоненты: смещение (bias), дисперсию (variance) и шум (noise). Эти компоненты позволяют описать сложность алгоритма, альтернативно сравнению ошибок на тренировочной и тестовой выборках. Хотя такое разложение можно построить для произвольной функции потерь, наиболее просто (и классически) оно строится для среднеквадратичной функции в задаче регрессии, что мы и рассмотрим ниже.\n",
    "\n",
    "Пусть $(X, y)$ — некоторая выборка. Обучим интересующий нас алгоритм на этой выборке и сделаем предсказания на ней. Обозначим предсказания как $\\hat{y}$. Тогда\n",
    "\n",
    "$$\n",
    "\\mathrm{bias} := \\mathbb{E}(\\hat{y}) - y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{variance} := \\mathbb{E}[\\mathbb{E}(\\hat{y}) - \\hat{y}]^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{noise} := \\mathbb{E}[y - \\mathbb{E}(y)]^2\n",
    "$$\n",
    "\n",
    "Ожидаемую среднеквадратичную ошибку на тренировочной выборке можно разложить как\n",
    "\n",
    "$$\n",
    "\\mathrm{E}[y - \\hat{y}]^2 = \\mathrm{bias}^2 + \\mathrm{variance} + \\mathrm{noise}.\n",
    "$$\n",
    "\n",
    "**Задание для самых смелых:** покажите, что это разложение корректно. Проверьте себя [здесь](https://github.com/esokolov/ml-course-hse/blob/master/2020-fall/lecture-notes/lecture08-ensembles.pdf).\n",
    "\n",
    "**Техническое замечание:** все математические ожидания в разложении выше берутся по объектам тренировочной выборки, то есть это разложение верно для среднеквадратичной ошибки на тренировочной выборке, которую иногда называют MSE for estimator. Тем не менее нам интересна и величина ошибки на ненаблюдаемых данных, которую иногда называют MSE for predictor. В этом случае математическое ожидание ошибки следует брать по ненаблюдаемым объектам. Для решения этой проблемы зачастую предполагается, что тренировочная и тестовая выборка имеют одинаковое распределение, и математическое ожидание берётся по всевозможным вариациям тренировочной выборки. Суть разложения при этом не изменится, однако запись его станет более громоздкой. Посмотреть на это можно [здесь](https://towardsdatascience.com/mse-and-bias-variance-decomposition-77449dd2ff55).\n",
    "\n",
    "Заметим, что так как на практике мы считаем оценки математических ожиданий и зачастую имеем доступ к тестовой выборке, то проблем с расчётом **оценок** MSE for estimator и MSE for predictor не возникает.\n",
    "\n",
    "Разберёмся с интерпретацией компонент:\n",
    "- $\\mathrm{Bias}$ — отклонение среднего ответа алгоритма от ответа идеального алгоритма. $\\mathrm{Bias}$ отражает ошибку модели, возникающую из-за простоты модели. Высокое смещение обычно является показателем того, что модель недообучена.\n",
    "- $\\mathrm{Variance}$ — разброс ответов алгоритмов относительно среднего ответа алгоритма. Показывает, насколько сильно небольшие изменения в обучающей выборке скажутся на предсказаниях алгоритма. $\\mathrm{Variance}$ отражает ошибку модели, возникающую из-за чрезмерной сложности модели. Высокая дисперсия обычно является показателем того, что модель переобучена.\n",
    "- $\\mathrm{Noise}$ — ошибка идеального классификатора, естественный неустранимый шум в данных.\n",
    "\n",
    "Посмотрим наглядно на примере полиномиальной регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:49:31.201889Z",
     "start_time": "2023-05-10T11:49:30.519955Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import trange\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:49:31.338511Z",
     "start_time": "2023-05-10T11:49:31.205037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Выборка\n",
    "np.random.seed(42)\n",
    "N = 10\n",
    "X = np.linspace(-5, 5, N).reshape(-1, 1)\n",
    "y = np.sin(X) + np.random.normal(0, 0.2, size=N).reshape(-1, 1)\n",
    "\n",
    "X_test = np.linspace(-5, 5, N // 2).reshape(-1, 1)\n",
    "y_test = np.sin(X_test) + np.random.normal(0, 0.2, size=N // 2).reshape(-1, 1)\n",
    "\n",
    "# Очень простая модель (регрессия на константу)\n",
    "too_simple_model_predictions = np.mean(y) * np.ones_like(y)\n",
    "\n",
    "# В меру сложная модель\n",
    "X_ok = np.hstack([X, X**2, X**3])\n",
    "ok_model = LinearRegression()\n",
    "ok_model.fit(X_ok, y)\n",
    "ok_model_predictions = ok_model.predict(X_ok)\n",
    "\n",
    "# Очень сложная модель\n",
    "X_compl = np.hstack(\n",
    "    [X, X**2, X**3, X**4, X**5, X**6, X**7, X**8, X**9, X**10]\n",
    ")\n",
    "compl_model = LinearRegression()\n",
    "compl_model.fit(X_compl, y)\n",
    "compl_model_predictions = compl_model.predict(X_compl)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.title(\"Сравнение моделей разной сложности\")\n",
    "plt.scatter(X, y, label=\"Тренировочная выборка\")\n",
    "plt.scatter(X_test, y_test, c=\"r\", label=\"Тестовая выборка\")\n",
    "plt.plot(X, too_simple_model_predictions, label=\"Очень простая модель\")\n",
    "plt.plot(X, ok_model_predictions, label=\"В меру сложная модель\")\n",
    "plt.plot(X, compl_model_predictions, label=\"Очень сложная модель\")\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** пользуясь определениями выше, объясните, наблюдаемые явления:\n",
    "\n",
    "- Очень простая модель имеет большое смещение (bias), но малую (нулевую) дисперсию (variance). Модель явно недообучена.\n",
    "- В меру сложная модель имеет небольшое смещение (bias) и небольшую дисперсию (variance).\n",
    "- Очень сложная модель имеет небольшое смещение (bias), но большую дисперсию (variance). Модель явно переобучена.\n",
    "\n",
    "**Задание:** прокомментируйте величину смещения и дисперсии для следующих моделей:\n",
    "\n",
    "- Линейная регрессия, обучаемая на большой выборке без выбросов и линейно зависимых признаков, в которой признаки сильно коррелируют с целевой переменной.\n",
    "- Решающее дерево, которое строится до тех пор, пока в листах не окажется по одному объекту.\n",
    "- Логистическая регрессия, относящая все точки к одному классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance tradeoff\n",
    "\n",
    "Из описания выше можно заметить, что при обучении моделей возникает выбор между смещением и дисперсией: недообученная модель имеет низкую дисперсию, но высокое смещение, а переобученная — низкое смещение, но высокую дисперсию. Этот выбор можно отобразить на картинке ([источник](https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update)). Здесь Total Error – ошибка на тестовой выборке (generalization error).\n",
    "\n",
    "![](https://www.bradyneal.com/img/bias-variance/fortmann-roe-bias-variance.png)\n",
    "\n",
    "Вывод из неё очевиден: строить следует оптимальные по сложности модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. От деревьев к случайному лесу\n",
    "\n",
    "### 2.1. Решающее дерево\n",
    "\n",
    "Мотивацию построения алгоритма случайного леса (Random Forest) удобно рассматривать в терминах смещения и дисперсии. Начнём с построения решающего дерева."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:49:31.346252Z",
     "start_time": "2023-05-10T11:49:31.338141Z"
    }
   },
   "outputs": [],
   "source": [
    "california = fetch_california_housing()\n",
    "california_X = pd.DataFrame(data=california.data, columns=california.feature_names)\n",
    "california_Y = california.target\n",
    "print(f\"X shape: {california_X.shape}, Y shape: {california_Y.shape}\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    california_X, california_Y, test_size=0.3, random_state=123, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:49:31.459233Z",
     "start_time": "2023-05-10T11:49:31.346360Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# TODO: обучите решающее дерево без ограничений на тренировочной выборке\n",
    "\n",
    "# TODO: рассчитайте MSE на тренировочной и тестовой выборках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:49:47.314276Z",
     "start_time": "2023-05-10T11:49:31.461093Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "\n",
    "# TODO: выведите среднее смещение и среднюю дисперсию модели на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы обсуждали на предыдущем семинаре, такое дерево окажется сильно переобученным (высокая дисперсия и низкое смещение). Постараемся исправить это. На лекции мы обсуждали, что один из способов борьбы с переобучением – построение композиций моделей. На этом семинаре мы рассмотрим построение композиций при помощи бэггинга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Бэггинг\n",
    "\n",
    "Вспомним суть алгоритма:\n",
    "1. Обучаем много деревьев на бутстрапированных подвыборках исходной выборки независимо друг от друга. Бутстрапированную подвыборку строим при помощи выбора $N$ (размер исходной выборки) наблюдений из исходной выборки с возвращением.\n",
    "2. Усредняем предсказания всех моделей (например, берём арифметическое среднее). \n",
    "\n",
    "Можно показать, что модель, построенная при помощи бэггинга, будет иметь **то же смещение**, что и у отдельных деревьев, но значительно **меньшую дисперсию** (при выполнении некоторых условий). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:50:53.873857Z",
     "start_time": "2023-05-10T11:49:47.316082Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "base_tree = DecisionTreeRegressor(random_state=123)\n",
    "\n",
    "# TODO: обучите бэггинг с 20 деревьями, каждое из которых строится без ограничений\n",
    "\n",
    "# TODO: рассчитайте MSE на тренировочной и тестовой выборках\n",
    "\n",
    "# TODO: выведите среднее смещение и среднюю дисперсию модели на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, по сравнению с единичным деревом смещение практически не изменилось, но дисперсия уменьшилась в несколько раз! Среднеквадратичная ошибка на тренировочной выборке больше не равна 0, а на тестовой — уменьшилась, что говорит о том, что мы успешно победили переобучение единичного решающего дерева.\n",
    "\n",
    "Можем ли мы снизить переобучение ещё сильнее? Можем!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Случайный лес\n",
    "\n",
    "При построении каждого дерева в бэггинге в ходе создания очередного узла будем выбирать случайный набор признаков, на основе которых производится разбиение. В результате такой процедуры мы уменьшим корреляцию между деревьями, за счёт чего снизим дисперсию итоговой модели. Такой алгоритм назвывается **случайным лесом** (Random Forest).\n",
    "\n",
    "По сравнению с единичным деревом к параметрам случайного леса добавляются:\n",
    "- `max_features` – число признаков, на основе которых проводятся разбиения при построении дерева.\n",
    "\n",
    "- `n_estimators` – число деревьев.\n",
    "\n",
    "Естественно, все параметры, относящиеся к единичному дереву, сохраняются для случайного леса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:51:54.113083Z",
     "start_time": "2023-05-10T11:50:53.876014Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# TODO: обучите случайный лес с 20 деревьями, каждое из которых строится без ограничений\n",
    "\n",
    "# TODO: рассчитайте MSE на тренировочной и тестовой выборках\n",
    "\n",
    "# TODO: выведите среднее смещение и среднюю дисперсию модели на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, по сравнению с бэггингом смещение вновь осталось практически неизменным, а дисперсия немного уменьшилась. Конечно, если подобрать хорошие гиперпараметры, то получится снизить дисперсию ещё больше. \n",
    "\n",
    "Ошибка на тренировочной выборке увеличилась, а на тестовой — уменьшилась, что означает, что мы добились нашей цели в борьбе с переобученными деревьями!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Особенности случайного леса\n",
    "\n",
    "### 3.1. Число деревьев и \"Случайный лес не переобучается\"\n",
    "\n",
    "В своём [блоге](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#remarks) Лео Бриман (Leo Breiman), создатель случайного леса, написал следующее:\n",
    "\n",
    "> Random forest does not overfit. You can run as many trees as you want.\n",
    "\n",
    "**Обратите внимание:** как говорилось на лекции, случайный лес не переобучается именно с ростом числа деревьев (за счёт совместной работы бэггинга и использования случайных подпространств), но не в принципе. Посмотрим на поведение случайного леса при росте числа деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:35.735267Z",
     "start_time": "2023-05-10T11:51:54.114270Z"
    }
   },
   "outputs": [],
   "source": [
    "n_trees = 100\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for i in trange(1, n_trees, 2):\n",
    "    rf = RandomForestRegressor(n_estimators=i, random_state=123, n_jobs=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_loss.append(mean_squared_error(y_train, rf.predict(X_train)))\n",
    "    test_loss.append(mean_squared_error(y_test, rf.predict(X_test)))\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Изменение лосса в зависимости от количества деревьев\")\n",
    "plt.grid()\n",
    "plt.plot(train_loss, label=\"MSE_train\")\n",
    "plt.plot(test_loss, label=\"MSE_test\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"# trees\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, по достижении некоторого числа деревьев обе ошибки практически не изменяются, то есть переобучения при росте числа деревьев не происходит. Однако практика показывает, что при изменении какого-нибудь другого параметра на реальных данных переобучение может произойти: [пример 1](https://datascience.stackexchange.com/questions/1028/do-random-forest-overfit), [пример 2](https://mljar.com/blog/random-forest-overfitting/). Например, случайный лес с ограниченными по глубине деревьями может предсказывать более точно, чем лес без ограничений. В нашем же случае случайный лес, скорее, лишь страдает от регуляризации. Например, посмотрим на поведение модели при изменении максимальной глубины деревьев (поэксперементируйте с другими параметрами)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:43.149372Z",
     "start_time": "2023-05-10T11:52:35.737047Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depth = 50\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for i in trange(1, max_depth, 2):\n",
    "    rf = RandomForestRegressor(n_estimators=20, max_depth=i, random_state=123, n_jobs=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_loss.append(mean_squared_error(y_train, rf.predict(X_train)))\n",
    "    test_loss.append(mean_squared_error(y_test, rf.predict(X_test)))\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Изменение лосса в зависимости от глубины деревьев\")\n",
    "plt.grid()\n",
    "plt.plot(train_loss, label=\"MSE_train\")\n",
    "plt.plot(test_loss, label=\"MSE_test\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переобучение не наблюдается. Вообще же, как обычно, гиперпараметры случайного леса стоит подбирать на кросс-валидации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Out-of-bag-ошибка\n",
    "\n",
    "Как мы обсудили выше, при построении случайного леса каждое дерево строится на бутстрапированной подвыборке, полученной из исходной обучающей выборки случайным набором с повторениями. Понятно, что некоторые наблюдения попадут в такую подвыборку несколько раз, а некоторые не войдут в неё вообще. Для каждого дерева мы можем рассмотреть объекты, которые не участвовали в обучении и использовать их для валидации.\n",
    "\n",
    "Усреднённая ошибка на неотобранных образцах по всему случайному лесу называется **out-of-bag-ошибкой**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:44.771132Z",
     "start_time": "2023-05-10T11:52:43.162809Z"
    }
   },
   "outputs": [],
   "source": [
    "# oob_score_ = R2 на невиденных наблюдениях\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=123, oob_score=True, n_jobs=4)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Важность признаков\n",
    "Как и решающие деревья, случайный лес позволяет оценивать важность признаков. Важность признаков можно оценивать, например, по тому, насколько сильно уменьшается хаотичность при применении предикатов на основе этого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:44.862097Z",
     "start_time": "2023-05-10T11:52:44.778753Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(california[\"feature_names\"], rf.feature_importances_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будьте осторожны с сильно коррелирующими признаками. Посмотрим, что произойдёт с важностью, если добавить в выборку линейно зависимый признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:44.865208Z",
     "start_time": "2023-05-10T11:52:44.863222Z"
    }
   },
   "outputs": [],
   "source": [
    "RM_mc = np.array((X_train.iloc[:, 5] * 2 + 3)).reshape(-1, 1)\n",
    "X_train_new = np.hstack((X_train, RM_mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:46.662664Z",
     "start_time": "2023-05-10T11:52:44.866029Z"
    }
   },
   "outputs": [],
   "source": [
    "rf.fit(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:46.743774Z",
     "start_time": "2023-05-10T11:52:46.664943Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "names = list(california[\"feature_names\"])\n",
    "names.append(\"_AveOccup\")\n",
    "plt.bar(names, rf.feature_importances_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важности перераспределились между линейной зависимыми признаками `AveOccup` и `_AveOccup`. Не забывайте учитывать корреляции между признаками, если вы используете этот метод для отбора признаков. Также обратите внимание на предупреждение в документации `sklearn`: не стоит использовать этот метод и для признаков, в которых есть много уникальных значений (например, категориальные признаки с небольшим числом категорий)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Тестирование случайного леса на разных данных\n",
    "Ниже представлены шаблоны для сравнения случайного леса и других моделей на данных разных типов. Проведите побольше экспериментов, используя разные модели и метрики. Попробуйте подобрать гиперпараметры случайного леса так, чтобы достичь какого-нибудь порога качества.\n",
    "\n",
    "**NB: Случайный лес может обучаться достаточно долго.**\n",
    "\n",
    "### 4.1. Бинарная классификация на примере [Kaggle Predicting a Biological Response](https://www.kaggle.com/c/bioresponse/data?select=train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:46.746170Z",
     "start_time": "2023-05-10T11:52:46.744028Z"
    }
   },
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "# !wget  -O 'kaggle_response.csv' -q 'https://www.dropbox.com/s/uha70sej5ugcrur/_train_sem09.csv?dl=1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:47.043457Z",
     "start_time": "2023-05-10T11:52:46.747687Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"kaggle_response.csv\")\n",
    "X = data.iloc[:, 1:].values\n",
    "y = data.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:47.915340Z",
     "start_time": "2023-05-10T11:52:47.044586Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# TODO: обучите логистическую регрессию и случайный лес с дефолтными параметрами\n",
    "# Сравните их AUC ROC на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Изображения на примере [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:48.522914Z",
     "start_time": "2023-05-10T11:52:47.916081Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:48.583920Z",
     "start_time": "2023-05-10T11:52:48.531764Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0, :]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:53:32.049629Z",
     "start_time": "2023-05-10T11:52:48.601595Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# TODO: обучите случайный лес и kNN с дефолтными параметрами\n",
    "# Сравните их доли правильных ответов на тестовой выборке"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
