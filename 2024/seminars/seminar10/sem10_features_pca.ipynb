{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lHox6jcTJJZ",
    "tags": []
   },
   "source": [
    "# Семинар\n",
    "\n",
    "## Отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lypxbB2qTJJe"
   },
   "source": [
    "Цель этого семинара — продемонстрировать несколько способов оценки важности признаков и их использования для отбора признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Qj5bVMYTJJf",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.testing as np_testing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa_oMjgiTJJh"
   },
   "source": [
    "# Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vHAu--ETJJi"
   },
   "source": [
    "## MAGIC – Major Atmospheric Gamma Imaging Cherenkov Telescope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp5bWsEGTJJi"
   },
   "source": [
    "MAGIC (Major Atmospheric Gamma Imaging Cherenkov) - это система, состоящая из двух черенковских телескопов диаметром 17 м. Они предназначены для наблюдения гамма-лучей от галактических и внегалактических источников в диапазоне очень высоких энергий (от 30 ГэВ до 100 ТэВ).\n",
    "\n",
    "Телескопами MAGIC в настоящее время управляют около 165 астрофизиков из 24 организаций и консорциумов из 12 стран. MAGIC позволил открыть и исследовать новые классы источников гамма-излучения, таких как, например, пульсары и гамма-всплески (GRB).\n",
    "\n",
    "<!--\n",
    "<center><img src=\"img/magic1.jpg\" width=\"1000\"></center>\n",
    "-->\n",
    "<center><img src=\"https://github.com/hse-ds/ml-hse-nes/blob/main/2023/seminars/seminar09/img/magic1.jpg?raw=true\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "Источник: https://magic.mpp.mpg.de/\n",
    "\n",
    "Youtube video: https://youtu.be/mjcDSR2vSU8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaEScCEeTJJj"
   },
   "source": [
    "## Частицы из космоса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQ-bB3QGTJJk"
   },
   "source": [
    "Космические частицы, $\\gamma$-кванты (фотоны) и адроны (протоны), взаимодействуют с атмосферой и порождают ливни вторичных частиц. Двигаясь с околосветовой скоростью, эти частицы излучают Черенковское излучение. Телескопы фотографируют это излучение. По фотографиям можно определить тип частицы из космоса: фотон или протон.\n",
    "\n",
    "<!--\n",
    "<center><img src=\"img/shower.jpg\" width=\"500\"></center>\n",
    "-->\n",
    "<center><img src=\"https://github.com/hse-ds/ml-hse-nes/blob/main/2023/seminars/seminar09/img/shower.jpg?raw=true\" alt=\"drawing\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItX-fcm3TJJk"
   },
   "source": [
    "## Фотографии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0hHVBIiTJJl"
   },
   "source": [
    "Задача атмосферного черенковского телескопа - получить изображение ливня путем измерения черенковского света от частиц ливня. Это изображение представляет собой геометрическую проекцию ливня на детектор. Для анализа этих изображений были введены параметры изображения или так называемые параметры Хилласа. Есть два вида параметров изображения: параметры формы и параметры ориентации. (Источник: http://ihp-lx.ethz.ch/Stamet/magic/parameters.html)\n",
    "\n",
    "<!--\n",
    "<center><img src=\"img/geo.jpg\" width=\"400\"></center>\n",
    "-->\n",
    "<center><img src=\"https://github.com/hse-ds/ml-hse-nes/blob/main/2023/seminars/seminar09/img/geo.jpg?raw=true\" alt=\"drawing\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7tClFGbTJJl"
   },
   "source": [
    "## Фотоны vs адроны"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1L76Fa1RTJJm"
   },
   "source": [
    "Изображения для $\\gamma$-квантов (фотонов) и адронов (протонов) отличаются по форме кластеров. Астрономы используют модели машинного обучения для классификации этих изображений. Для обучения моделей ученые искусственно генерируют такие изображения для каждого типа частиц с помощью сложных физических симуляторов.\n",
    "\n",
    "<!--\n",
    "<center><img src=\"img/gamma_p.png\" width=\"600\"></center>\n",
    "-->\n",
    "<center><img src=\"https://github.com/hse-ds/ml-hse-nes/blob/main/2023/seminars/seminar09/img/gamma_p.png?raw=true\" alt=\"drawing\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnhFbSrDTJJn"
   },
   "source": [
    "# Part 1: Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BlUCPbBTJJn"
   },
   "source": [
    "### Data preparation\n",
    "\n",
    "UCI MAGIC dataset: https://archive.ics.uci.edu/ml/datasets/magic+gamma+telescope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRY1PlZvTJJn"
   },
   "source": [
    "Features description:\n",
    "- **Length:** continuous # major axis of ellipse [mm]\n",
    "- **Width:** continuous # minor axis of ellipse [mm]\n",
    "- **Size:** continuous # 10-log of sum of content of all pixels [in #phot]\n",
    "- **Conc:** continuous # ratio of sum of two highest pixels over fSize [ratio]\n",
    "- **Conc1:** continuous # ratio of highest pixel over fSize [ratio]\n",
    "- **Asym:** continuous # distance from highest pixel to center, projected onto major axis [mm]\n",
    "- **M3Long:** continuous # 3rd root of third moment along major axis [mm]\n",
    "- **M3Trans:** continuous # 3rd root of third moment along minor axis [mm]\n",
    "- **Alpha:** continuous # angle of major axis with vector to origin [deg]\n",
    "- **Dist:** continuous # distance from origin to center of ellipse [mm]\n",
    "- **Label:** g,h # gamma (signal), hadron (background)\n",
    "\n",
    "g = gamma (signal): 12332 \\\n",
    "h = hadron (background): 6688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1731328703599,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "Do2cbjlsTJJo",
    "outputId": "faf3cbcf-9b55-445c-bb0e-564329fcad09"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/hse-ds/ml-hse-nes/main/2022/seminars/seminar08/data/MAGIC/magic04.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1731328706032,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "OSDAxG_2TJJp",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "1cbb1674-7365-4ab3-d830-fff71d196b81"
   },
   "outputs": [],
   "source": [
    "f_names = np.array([\"Length\", \"Width\", \"Size\", \"Conc\", \"Conc1\", \"Asym\", \"M3Long\", \"M3Trans\", \"Alpha\", \"Dist\"])\n",
    "\n",
    "data = pd.read_csv(\"magic04.data\", header=None, names=list(f_names)+[\"Label\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxGEMyh5TJJq",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# prepare a matrix of input features\n",
    "X = data[f_names].values\n",
    "\n",
    "# prepare a vector of true labels\n",
    "y = 1 * (data['Label'].values == \"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOTk5zomTJJq",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into train and test subsamples to fit and test classifiers\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=11, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I33Y9liMTJJr"
   },
   "source": [
    "## Gradient Boosting based feature importances\n",
    "\n",
    "<!--\n",
    "<center><img src=\"img/tree.png\" width=\"400\"></center>\n",
    "-->\n",
    "<center><img src=\"https://github.com/hse-ds/ml-hse-nes/blob/main/2023/seminars/seminar09/img/tree.png?raw=true\" alt=\"drawing\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peN5DDHcTJJr"
   },
   "source": [
    "Let $T(f)$ be the set of all nodes which use feature $f$ to make split. Then, feature importance $Imp(f)$ of $f$:\n",
    "\n",
    "$$\n",
    "Imp(f) = \\sum_{t \\in T(f)} n_t \\Delta I(t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta I(t) = I(t) - \\sum_{c \\in children} \\frac{n_c}{n_t} I(c)\n",
    "$$\n",
    "\n",
    "where\n",
    "- $n_{t}$ - number of objects in node $t$;\n",
    "- $I(t)$ – impurity function (gini, cross-entropy, MSE) value for the node.\n",
    "\n",
    "Feature importances estimated by each tree in an ensemble are averaged over all trees in this ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oerqI_jhTJJr",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# this function will be used just to plot feature importances\n",
    "\n",
    "def plot_feature_importances(f_imps, f_names, title=\"\"):\n",
    "    f_imps = np.array(f_imps)\n",
    "    f_names = np.array(f_names)\n",
    "    sort_inds = np.argsort(f_imps)\n",
    "    yy = np.arange(len(f_imps)).astype(int)\n",
    "    plt.barh(yy, f_imps[sort_inds])\n",
    "    plt.yticks(yy, f_names[sort_inds], size=14)\n",
    "    plt.xticks(size=14)\n",
    "    plt.xlabel(\"Feature importance\", size=14)\n",
    "    plt.title(title, size=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5327,
     "status": "ok",
     "timestamp": 1731328748769,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "yk_hNe53TJJs",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "988ea349-6dec-4953-cd08-c0fe68dff9e3"
   },
   "outputs": [],
   "source": [
    "# import quality metrics and GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# define a classifier\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=11)\n",
    "\n",
    "# fit it using the train subsample\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# get predictions for the test subsample\n",
    "y_test_proba = gb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# compute roc auc score on the test\n",
    "roc_auc_gb = metrics.roc_auc_score(y_test, y_test_proba)\n",
    "print(\"Test ROC AUC: \", roc_auc_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K0uCqSkTJJs",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get feature imporatnces\n",
    "f_imps_gb = gb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1731328767811,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "nrkyffLeTJJt",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "1c82e3fd-d47b-4fd4-966a-0ec6902ca6a9"
   },
   "outputs": [],
   "source": [
    "# display the feature importances\n",
    "plot_feature_importances(f_imps_gb, f_names, \"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yUCEkxtTJJt"
   },
   "source": [
    "## Linear model based feature importances\n",
    "\n",
    "Consider a linear model with regularization ($L_1$ or $L_2$ penalty):\n",
    "\n",
    "$$\n",
    "\\hat{y}=w_0 + w_1 f_1+ w_2 f_2 + ... + w_k f_k\n",
    "$$\n",
    "\n",
    "If features are normalized (have the same ranges), feature importance $Imp(f_i)$ of $f_i$ is equal to:\n",
    "\n",
    "$$\n",
    "Imp(f_i) = | w_i |\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n9IdljPTJJu"
   },
   "source": [
    "### Task 1\n",
    "Estimate feature importacnes using linear model as it is described above.\n",
    "\n",
    "**Hints:** use `StandardScaler()` to normalize feature values. Also, use `LogisticRegression(solver='liblinear', penalty='l2', C=C, random_state=11)` for the linear model. To get values of the model coefficients use `<model>.coef_[0]` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jsq5_DiKTJJu",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "ba9090_ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_feature_imp_with_lin_mod(X_train, y_train, C=1.0):\n",
    "    \"\"\"\n",
    "    Estimate feature importances using linear model with regularization.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train: numpy.ndarray\n",
    "        Object features matrix.\n",
    "    y_train: numpy.array\n",
    "        Vector of true class labels.\n",
    "    C: float\n",
    "        Inverse of regularization strength; must be a positive float.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    f_imps_lin: numpy.array\n",
    "        Estimated feature importances.\n",
    "    \"\"\"\n",
    "\n",
    "    # normalize feature values\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_train)\n",
    "    X_train_ss = ss.transform(X_train)\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    ...\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return f_imps_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1731328871963,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "RS3jhOxtTJJu",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "45743fd9-e6ff-42e5-e571-4d9c142923e8"
   },
   "outputs": [],
   "source": [
    "f_imps_lin = get_feature_imp_with_lin_mod(X_train, y_train, C=1.)\n",
    "f_imps_lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyydIKyHTJJv"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "<center>   \n",
    "    \n",
    "```python\n",
    "array([1.23276898, 0.08849417, 0.31007411, 0.07866152, 0.69042639,\n",
    "       0.01030818, 0.31858178, 0.01445776, 1.20855327, 0.0711564 ])\n",
    "    \n",
    "```\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhpP8lLRTJJw",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "ba9090",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "actual  = f_imps_lin\n",
    "desired = np.array([1.23276898, 0.08849417, 0.31007411, 0.07866152, 0.69042639,\n",
    "                    0.01030818, 0.31858178, 0.01445776, 1.20855327, 0.0711564 ])\n",
    "np_testing.assert_almost_equal(actual, desired, decimal=3)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 949
    },
    "executionInfo": {
     "elapsed": 618,
     "status": "ok",
     "timestamp": 1731328878862,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "VYbtzQbeTJJx",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6ba6594f-1370-4e54-ba2f-63e8f03a3034"
   },
   "outputs": [],
   "source": [
    "# display the feature importances\n",
    "plot_feature_importances(f_imps_lin, f_names, \"Linear model\")\n",
    "plot_feature_importances(f_imps_gb, f_names, \"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVbN-B34TJJy"
   },
   "source": [
    "Do you have any ideas why the feature importances are so different for these two models? Let's compare quality of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1731328890164,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "OBfj9FsJTJJy",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "8595da78-14e1-4461-8ee1-4e9cac2752de"
   },
   "outputs": [],
   "source": [
    "# normalize feature values\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_ss = ss.transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)\n",
    "\n",
    "# fit a linear model with regularization\n",
    "linclf = LogisticRegression(solver='liblinear', penalty='l2', C=1.0, random_state=11)\n",
    "linclf.fit(X_train_ss, y_train)\n",
    "\n",
    "# get predictions for the test subsample\n",
    "y_test_proba = linclf.predict_proba(X_test_ss)[:, 1]\n",
    "\n",
    "# compute roc auc score on the test\n",
    "roc_auc_lin = metrics.roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "print(\"Test ROC AUC (GB)    : \", roc_auc_gb)\n",
    "print(\"Test ROC AUC (LogReg): \", roc_auc_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pf7wMAZTJJz"
   },
   "source": [
    "## General method\n",
    "\n",
    "<!--\n",
    "<center><img src=\"img/general.png\" width=\"500\"></center>\n",
    "-->\n",
    "<center><img src=\"https://github.com/hse-ds/ml-hse-nes/blob/main/2023/seminars/seminar09/img/general.png?raw=true\" alt=\"drawing\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eHQS8bfTJJ0"
   },
   "source": [
    "Algorithm:\n",
    "- Train your model\n",
    "- Calculate quality measure $Q_0$ on the test set\n",
    "- For a feature $f$:\n",
    " - Replace given values with random values from the same distribution (perform random shuffling)\n",
    " - Calculate quality measure $Q_f$ on the test set\n",
    " - Estimate feature importance: $Imp(f)=Q_0 - Q_f$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6HFfPmATJJ0"
   },
   "source": [
    "### Task 2\n",
    "\n",
    "Estimate feature importances using general algorithm described above.\n",
    "\n",
    "**Hint:** to shuffle values of one feature use `numpy.random.RandomState(42).shuffle()`, for an example: `X[:, i] = np.random.RandomState(42).shuffle(X[:, i])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "executionInfo": {
     "elapsed": 5076,
     "status": "ok",
     "timestamp": 1731328935221,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "R8MWLaYjTJJ1",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "d4520da3-9309-4ab0-9bcd-d2897eed5946"
   },
   "outputs": [],
   "source": [
    "# define a model that we will use to estimate feature importances\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=11)\n",
    "\n",
    "# fit the model on the train sample\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1731328935222,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "J7ARX_E3TJJ2",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6e2c4705-6291-42ad-e73c-02e3001875e4"
   },
   "outputs": [],
   "source": [
    "model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n00iaP5-TJJ2",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "1df626_ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_feature_imp_general(X_test, y_test, model):\n",
    "    \"\"\"\n",
    "    Estimate feature importances using linear model with regularization.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_test: numpy.ndarray\n",
    "        Object features matrix.\n",
    "    y_test: numpy.array\n",
    "        Vector of true class labels.\n",
    "    model: object\n",
    "        A classifier fitted on the train sample\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    f_imps_gen: numpy.array\n",
    "        Estimated feature importances.\n",
    "    \"\"\"\n",
    "\n",
    "    # define a list for the feature importances\n",
    "    f_imps_gen = []\n",
    "\n",
    "    # calculate the base quality value according to the algorithm\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    q_0 = metrics.roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "    # for each feature in the sample estimate its importance\n",
    "    for i in range(X_test.shape[1]):\n",
    "\n",
    "        # do not forget to make a copy of X_test!\n",
    "        X_test_copy = X_test.copy()\n",
    "\n",
    "        # shuffle values of the i-th feature\n",
    "        ### BEGIN SOLUTION\n",
    "        ...\n",
    "        ### END SOLUTION\n",
    "\n",
    "        # calculate quality metric value\n",
    "        X_test_copy = np.nan_to_num(X_test_copy)\n",
    "        y_test_proba = model.predict_proba(X_test_copy)[:, 1]\n",
    "        q_f = metrics.roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "        # estimate importance of the feature\n",
    "        imp = q_0 - q_f\n",
    "        f_imps_gen.append(imp)\n",
    "\n",
    "    return np.array(f_imps_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 521,
     "status": "ok",
     "timestamp": 1731328950680,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "71ywJPy1TJJ3",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "92806b97-ce6a-4625-e271-b519b9b89792"
   },
   "outputs": [],
   "source": [
    "f_imps_gen = get_feature_imp_general(X_test, y_test, model)\n",
    "f_imps_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGpWoIJTTJJ4"
   },
   "source": [
    "Expected output (approximately):\n",
    "\n",
    "<center>   \n",
    "    \n",
    "```python\n",
    "array([0.04537013, 0.06596446, 0.08269023, 0.00817734, 0.00350842,\n",
    "       0.00091059, 0.00249184, 0.00053674, 0.11450679, 0.09304359])\n",
    "    \n",
    "```\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWqX655VTJJ4",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "1df626",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "actual  = f_imps_gen\n",
    "desired = np.array([0.04537013, 0.06596446, 0.08269023, 0.00817734, 0.00350842,\n",
    "                    0.00091059, 0.00249184, 0.00053674, 0.11450679, 0.09304359])\n",
    "np_testing.assert_almost_equal(actual, desired, decimal=2)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 949
    },
    "executionInfo": {
     "elapsed": 618,
     "status": "ok",
     "timestamp": 1731329081076,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "Pi6IqhCCTJJ4",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "156fd9b2-7115-4796-d934-ba7e63ee91f8"
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(f_imps_gen, f_names, \"General\")\n",
    "plot_feature_importances(f_imps_gb, f_names, \"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IKmE5oLTJJ5"
   },
   "source": [
    "# Part 2: Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miFFVTK4TJJ5"
   },
   "source": [
    "## Recursive feature elimination\n",
    "\n",
    "- Train a model on the full set of features\n",
    "- Estimate feature importance (based on the model)\n",
    "- Remove the least important feature\n",
    "- Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQyKJPBvTJJ5"
   },
   "source": [
    "### Task 3\n",
    "Implement recursive feature elimination using `model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=11)` as a model.\n",
    "\n",
    "**Hint:** use feature importances estimated by the model `model.feature_importances_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29547,
     "status": "ok",
     "timestamp": 1731329116371,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "b3EJrQ4yTJJ6",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "70c60f_ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "98811e02-3956-4eb9-884d-b4560f094772"
   },
   "outputs": [],
   "source": [
    "# make copies for further steps\n",
    "X_train_curr = X_train.copy()\n",
    "X_test_curr  = X_test.copy()\n",
    "f_names_curr = f_names.copy()\n",
    "\n",
    "# for storing roc auc scores\n",
    "roc_auc_scores = []\n",
    "\n",
    "# eliminate feature by feature\n",
    "for i in range(X.shape[1]):\n",
    "\n",
    "    print(\"Features: \", f_names_curr)\n",
    "\n",
    "    # 1. fit the model using current set of festures\n",
    "    # 2. get feature importances\n",
    "    ### BEGIN SOLUTION\n",
    "    ...\n",
    "    ### END SOLUTION\n",
    "\n",
    "    # compute roc auc of the model\n",
    "    y_test_proba = model.predict_proba(X_test_curr)[:, 1]\n",
    "    auc = metrics.roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "    # print and store it\n",
    "    auc = np.round(auc, 4)\n",
    "    print(\"ROC AUC: \", auc)\n",
    "    roc_auc_scores.append(auc)\n",
    "\n",
    "    # remove feature with the least importance\n",
    "    X_train_curr = X_train_curr[:, f_imps > f_imps.min()]\n",
    "    X_test_curr  = X_test_curr[:, f_imps > f_imps.min()]\n",
    "    f_names_curr = f_names_curr[f_imps > f_imps.min()]\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"Output: \", roc_auc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFHpFpNWTJJ6"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "<center>   \n",
    "    \n",
    "```python\n",
    "[0.9235, 0.9232, 0.9229, 0.9218, 0.9187, 0.9153, 0.9113, 0.8862, 0.8666, 0.7823]\n",
    "    \n",
    "```\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78nsdfoUTJJ7",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "70c60f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "actual  = roc_auc_scores\n",
    "desired = [0.9235, 0.9232, 0.9229, 0.9218, 0.9187, 0.9153, 0.9113, 0.8862, 0.8666, 0.7823]\n",
    "np_testing.assert_almost_equal(actual, desired, decimal=3)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1731329116372,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "FR9oK4bJTJJ7",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "adbe07bb-1b0a-46f9-ac24-720388a75825"
   },
   "outputs": [],
   "source": [
    "nf = np.arange(1, len(roc_auc_scores)+1)[::-1]\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(nf, roc_auc_scores, linewidth=3)\n",
    "plt.scatter(nf, roc_auc_scores, linewidth=3)\n",
    "\n",
    "plt.xlabel(\"Number of features\", size=16)\n",
    "plt.ylabel(\"ROC AUC\", size=16)\n",
    "plt.xticks(size=16)\n",
    "plt.yticks(size=16)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPz5drzxuil3"
   },
   "source": [
    "# Часть 3. Метод главных компонент (Principal Component Analysis, PCA)\n",
    "\n",
    "[Ноутбук Евгения Соколова](https://github.com/esokolov/ml-course-hse/blob/master/2022-spring/seminars/sem14_pca_tsne.ipynb)\n",
    "\n",
    "Выделение новых признаков путем их отбора часто дает плохие результаты, и в некоторых ситуациях такой подход практически бесполезен. Например, если мы работаем с изображениями, у которых признаками являются яркости пикселей,\n",
    "невозможно выбрать небольшой поднабор пикселей, который дает хорошую информацию о содержимом картинки. Поэтому признаки нужно как-то комбинировать.\n",
    "\n",
    "__Метод главных компонент__ &mdash; один из самых интуитивно простых и часто используемых методов для снижения размерности данных и проекции их на ортогональное подпространство признаков. В рамках метода делается два важных упрощения задачи\n",
    "\n",
    "1. игнорируется целевая переменная;\n",
    "2. строится линейная комбинация признаков.\n",
    "\n",
    "П. 1 на первый взгляд кажется довольно странным, но на практике обычно не является таким уж плохим. Это связано с тем, что часто данные устроены так, что имеют какую-то внутреннюю структуру в пространстве меньшей размерности, которая никак не связана с целевой переменной. Поэтому и оптимальные признаки можно строить не глядя на ответ.\n",
    "\n",
    "П. 2 тоже сильно упрощает задачу, но далее мы научимся избавляться от него."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcsASJ4Iuil3"
   },
   "source": [
    "### Теория\n",
    "\n",
    "Кратко вспомним, что делает этот метод (подробно см. в лекции).\n",
    "\n",
    "Пусть $X$ &mdash; матрица объекты-признаки, с нулевым средним каждого признака, а $w$ &mdash; некоторый единичный вектор. Тогда $Xw$ задает величину проекций всех объектов на этот вектор. Далее ищется вектор, который дает наибольшую дисперсию полученных проекций (то есть наибольшую дисперсию вдоль этого направления):\n",
    "\n",
    "$$\n",
    "\\max_{w: \\|w\\|=1} \\| Xw \\|^2 =  \\max_{w: \\|w\\|=1} w^T X^T X w\n",
    "$$\n",
    "\n",
    "Подходящий вектор тогда равен собственному вектору матрицы $X^T X$ с наибольшим собственным значением. После этого все пространство проецируется на ортогональное дополнение к вектору $w$ и процесс повторяется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJGHeSKVuil4"
   },
   "source": [
    "## 3.1 PCA на плоскости\n",
    "\n",
    "Для начала посмотрим на метод PCA на плоскости для того, чтобы лучше понять, как он устроен. Попробуем специально сделать один из признаков более значимым и проверим, что PCA это обнаружит. Сгенерируем выборку из двухмерного нормального распределения с нулевым математическим ожиданием."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0j0T6SM7uil4"
   },
   "outputs": [],
   "source": [
    "np.random.seed(314512)\n",
    "\n",
    "data_synth_1 = np.random.multivariate_normal(\n",
    "    mean=[0, 0],\n",
    "    cov=[[4, 0],\n",
    "         [0, 1]],\n",
    "    size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyP0H4D8uil4"
   },
   "source": [
    "Теперь изобразим точки выборки на плоскости и применим к ним PCA для нахождения главных компонент. В результате работы PCA из sklearn в `dec.components_` будут лежать главные направления (нормированные), а в `dec.explained_variance_` &mdash; дисперсия, которую объясняет каждая компонента. Изобразим на нашем графике эти направления, умножив их на дисперсию для наглядного отображения их значимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1731329126753,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "HgLaaRjyuil5",
    "outputId": "86706b6e-7de7-4db6-a867-891d1853e978"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def PCA_show(dataset):\n",
    "    plt.scatter(*zip(*dataset), alpha=0.5)\n",
    "\n",
    "    dec = PCA()\n",
    "    dec.fit(dataset)\n",
    "    ax = plt.gca()\n",
    "    for comp_ind in range(dec.components_.shape[0]):\n",
    "        component = dec.components_[comp_ind, :]\n",
    "        var = dec.explained_variance_[comp_ind]\n",
    "        start, end = dec.mean_, component * var\n",
    "        ax.arrow(start[0], start[1], end[0], end[1],\n",
    "                 head_width=0.2, head_length=0.4, fc='r', ec='r')\n",
    "\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "PCA_show(data_synth_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euprFbbouil5"
   },
   "source": [
    "Видим, что PCA все правильно нашел. Но это, конечно, можно было сделать и просто посчитав\n",
    "дисперсию каждого признака. Повернем наши данные на некоторый фиксированный угол и проверим,\n",
    "что для PCA это ничего не изменит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1731329134681,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "skVEV-vouil6",
    "outputId": "1031b12e-6203-4970-d149-bc21a960d440"
   },
   "outputs": [],
   "source": [
    "angle = np.pi / 6\n",
    "rotate = np.array([\n",
    "        [np.cos(angle), - np.sin(angle)],\n",
    "        [np.sin(angle), np.cos(angle)],\n",
    "    ])\n",
    "data_synth_2 = rotate.dot(data_synth_1.T).T\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "PCA_show(data_synth_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfbQz3Lhuil6"
   },
   "source": [
    "Ниже пара примеров, где PCA отработал не так хорошо (в том смысле, что направления задают не очень хорошие признаки).\n",
    "\n",
    "**Упражнение:** объясните, почему так произошло."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "executionInfo": {
     "elapsed": 1845,
     "status": "ok",
     "timestamp": 1731329144153,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "rbyVfQ5Puil6",
    "outputId": "7a656da9-9fd4-42c8-9ff8-c0f29ef51520",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles, make_moons, make_blobs\n",
    "\n",
    "np.random.seed(54242)\n",
    "\n",
    "data_synth_bad = [\n",
    "    make_circles(n_samples=1000, factor=0.2, noise=0.1)[0]*2,\n",
    "    make_moons(n_samples=1000, noise=0.1)[0]*2,\n",
    "    make_blobs(n_samples=1000, n_features=2, centers=4)[0]/5,\n",
    "    np.random.multivariate_normal(\n",
    "        mean=[0, 1.5],\n",
    "        cov=[[3, 1],\n",
    "             [1, 1]],\n",
    "        size=1000),\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "rows, cols = 2, 2\n",
    "for i, data in enumerate(data_synth_bad):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    PCA_show(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsxGXRAguil7"
   },
   "source": [
    "## 2.2 Лица людей\n",
    "\n",
    "Рассмотрим датасет с фотографиями лиц людей и применим к его признакам PCA. Ниже изображены примеры лиц из базы, и последняя картинка &mdash; это \"среднее лицо\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "executionInfo": {
     "elapsed": 3058,
     "status": "ok",
     "timestamp": 1731329162353,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "Y8mUGTKtuil7",
    "outputId": "382ff606-5ca3-47df-e009-922eb97f22c7"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "faces = fetch_olivetti_faces(shuffle=True, random_state=432542)\n",
    "faces_images = faces.data\n",
    "faces_ids = faces.target\n",
    "image_shape = (64, 64)\n",
    "\n",
    "mean_face = faces_images.mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "rows, cols = 2, 4\n",
    "n_samples = rows * cols\n",
    "for i in range(n_samples - 1):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(faces_images[i, :].reshape(image_shape), interpolation='none',\n",
    "               cmap='gray')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "plt.subplot(rows, cols, n_samples)\n",
    "plt.imshow(mean_face.reshape(image_shape), interpolation='none',\n",
    "           cmap='gray')\n",
    "plt.xticks(())\n",
    "_ = plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwAGKB8huil7"
   },
   "source": [
    "Теперь найдем главные компоненты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "executionInfo": {
     "elapsed": 2028,
     "status": "ok",
     "timestamp": 1731329181809,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "X146vBs2uil7",
    "outputId": "33721c26-1081-4452-939b-71c067099ad4"
   },
   "outputs": [],
   "source": [
    "model_pca = PCA()\n",
    "faces_images -= mean_face  # отнормировали данные к нулевому среднему\n",
    "model_pca.fit(faces_images)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "rows, cols = 2, 4\n",
    "n_samples = rows * cols\n",
    "for i in range(n_samples):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(model_pca.components_[i, :].reshape(image_shape), interpolation='none', cmap='gray')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftqUOZN1uil8"
   },
   "source": [
    "Получилось жутковато, что уже неплохо, но есть ли от этого какая-то польза?\n",
    "\n",
    "- Во-первых, новые признаки дают более высокое качество классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P49S722Tuil8"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gscv_rf = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    {'n_estimators': [100, 200, 500, 800], 'max_depth': [2, 3, 4, 5]}, cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286678,
     "status": "ok",
     "timestamp": 1731329472521,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "Hw2rBOI8uil8",
    "outputId": "d6a2344c-fc9c-4b5e-e33f-72a673027539"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gscv_rf.fit(faces_images, faces_ids)\n",
    "print(gscv_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107050,
     "status": "ok",
     "timestamp": 1731329579566,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "iRG4Cnyhuil8",
    "outputId": "5c491186-b1b9-4cd5-ee4c-71796327dd13"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gscv_rf.fit(model_pca.transform(faces_images)[:,:100], faces_ids)\n",
    "print(gscv_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8H_CqZLuil9"
   },
   "source": [
    "На практике можно выбирать столько главных компонент, чтобы оставить $90\\%$ дисперсии исходных данных. В данном случае для этого достаточно выделить около $60$ главных компонент, то есть снизить размерность с $4096$ признаков до $60$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1731329579566,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "6yCEpxrAuil9",
    "outputId": "f59c8a3a-c9a6-435b-d24a-d3e5ac111cc9"
   },
   "outputs": [],
   "source": [
    "faces_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1731329624908,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "oJVPu3B1uil9",
    "outputId": "2e5406fe-b224-4d32-b09f-59196618f416"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(np.cumsum(model_pca.explained_variance_ratio_), color='k', lw=2)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.xlim(0, 63)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.axvline(21, c='b')\n",
    "plt.axhline(0.9, c='r')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH9FhLMuuil9"
   },
   "source": [
    " - Во-вторых, их можно использовать для компактного хранения данных. Для этого объекты трансформируются в новое пространство, и из него выкидываются самые незначимые признаки. Ниже приведены результаты сжатия в 20 раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "executionInfo": {
     "elapsed": 2696,
     "status": "ok",
     "timestamp": 1731329631537,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "EKPJL0_Duil-",
    "outputId": "c7f51a7c-e08d-4b5c-b412-78e7761eab48"
   },
   "outputs": [],
   "source": [
    "base_size = image_shape[0] * image_shape[1]\n",
    "\n",
    "def compress_and_show(compress_ratio):\n",
    "    model_pca = PCA(n_components=int(base_size * compress_ratio))\n",
    "    model_pca.fit(faces_images)\n",
    "\n",
    "    faces_compressed = model_pca.transform(faces_images)\n",
    "\n",
    "    # обратное преобразование\n",
    "    faces_restored = model_pca.inverse_transform(faces_compressed) + mean_face\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    rows, cols = 2, 4\n",
    "    n_samples = rows * cols\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(faces_restored[i, :].reshape(image_shape), interpolation='none',\n",
    "                   cmap='gray')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "compress_and_show(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "528KQS_Zuil-"
   },
   "source": [
    "И даже при сжатии в 50 раз лица остаются узнаваемыми."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "executionInfo": {
     "elapsed": 1280,
     "status": "ok",
     "timestamp": 1731329633084,
     "user": {
      "displayName": "Sergey Korpachev",
      "userId": "09181340988160569540"
     },
     "user_tz": -180
    },
    "id": "MgowiDZMuil-",
    "outputId": "5ad348db-5af2-4b6a-defc-786be1ef93e9"
   },
   "outputs": [],
   "source": [
    "compress_and_show(0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGS2Rqh5Za7Y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
