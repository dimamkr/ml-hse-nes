{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFxyWFdDNpt7"
      },
      "source": [
        "# Семинар 7: Случайный лес\n",
        "[источник](https://github.com/hse-ds/iad-intro-ds/tree/master/2023/seminars/sem13_rf)\n",
        "## 1. Bias-Variance decomposition\n",
        "\n",
        "Вспомним, что функцию потерь в задачах регрессии или классификации можно разложить на три компоненты: смещение (bias), дисперсию (variance) и шум (noise). Эти компоненты позволяют описать сложность алгоритма, альтернативно сравнению ошибок на тренировочной и тестовой выборках. Хотя такое разложение можно построить для произвольной функции потерь, наиболее просто (и классически) оно строится для среднеквадратичной функции в задаче регрессии, что мы и рассмотрим ниже.\n",
        "\n",
        "Пусть $(X, y)$ — некоторая выборка. Обучим интересующий нас алгоритм на этой выборке и сделаем предсказания на ней. Обозначим предсказания как $\\hat{y}$. Тогда\n",
        "\n",
        "$$\n",
        "\\mathrm{bias} := \\mathbb{E}(\\hat{y}) - y\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathrm{variance} := \\mathbb{E}[\\mathbb{E}(\\hat{y}) - \\hat{y}]^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathrm{noise} := \\mathbb{E}[y - \\mathbb{E}(y)]^2\n",
        "$$\n",
        "\n",
        "Ожидаемую среднеквадратичную ошибку на тренировочной выборке можно разложить как\n",
        "\n",
        "$$\n",
        "\\mathrm{E}[y - \\hat{y}]^2 = \\mathrm{bias}^2 + \\mathrm{variance} + \\mathrm{noise}.\n",
        "$$\n",
        "\n",
        "**Задание для самых смелых:** покажите, что это разложение корректно. Проверьте себя [здесь](https://github.com/esokolov/ml-course-hse/blob/master/2020-fall/lecture-notes/lecture08-ensembles.pdf).\n",
        "\n",
        "**Техническое замечание:** все математические ожидания в разложении выше берутся по объектам тренировочной выборки, то есть это разложение верно для среднеквадратичной ошибки на тренировочной выборке, которую иногда называют MSE for estimator. Тем не менее нам интересна и величина ошибки на ненаблюдаемых данных, которую иногда называют MSE for predictor. В этом случае математическое ожидание ошибки следует брать по ненаблюдаемым объектам. Для решения этой проблемы зачастую предполагается, что тренировочная и тестовая выборка имеют одинаковое распределение, и математическое ожидание берётся по всевозможным вариациям тренировочной выборки. Суть разложения при этом не изменится, однако запись его станет более громоздкой. Посмотреть на это можно [здесь](https://towardsdatascience.com/mse-and-bias-variance-decomposition-77449dd2ff55).\n",
        "\n",
        "Заметим, что так как на практике мы считаем оценки математических ожиданий и зачастую имеем доступ к тестовой выборке, то проблем с расчётом **оценок** MSE for estimator и MSE for predictor не возникает.\n",
        "\n",
        "Разберёмся с интерпретацией компонент:\n",
        "- $\\mathrm{Bias}$ — отклонение среднего ответа алгоритма от ответа идеального алгоритма. $\\mathrm{Bias}$ отражает ошибку модели, возникающую из-за простоты модели. Высокое смещение обычно является показателем того, что модель недообучена.\n",
        "- $\\mathrm{Variance}$ — разброс ответов алгоритмов относительно среднего ответа алгоритма. Показывает, насколько сильно небольшие изменения в обучающей выборке скажутся на предсказаниях алгоритма. $\\mathrm{Variance}$ отражает ошибку модели, возникающую из-за чрезмерной сложности модели. Высокая дисперсия обычно является показателем того, что модель переобучена.\n",
        "- $\\mathrm{Noise}$ — ошибка идеального классификатора, естественный неустранимый шум в данных.\n",
        "\n",
        "Посмотрим наглядно на примере полиномиальной регрессии."
      ],
      "id": "ZFxyWFdDNpt7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:49:31.201889Z",
          "start_time": "2023-05-10T11:49:30.519955Z"
        },
        "id": "x7PaStQFNpt_"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import trange\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "x7PaStQFNpt_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:49:31.338511Z",
          "start_time": "2023-05-10T11:49:31.205037Z"
        },
        "id": "BTXQwUbTNpuB"
      },
      "outputs": [],
      "source": [
        "# Выборка\n",
        "np.random.seed(42)\n",
        "N = 10\n",
        "X = np.linspace(-5, 5, N).reshape(-1, 1)\n",
        "y = np.sin(X) + np.random.normal(0, 0.2, size=N).reshape(-1, 1)\n",
        "\n",
        "X_test = np.linspace(-5, 5, N // 2).reshape(-1, 1)\n",
        "y_test = np.sin(X_test) + np.random.normal(0, 0.2, size=N // 2).reshape(-1, 1)\n",
        "\n",
        "# Очень простая модель (регрессия на константу)\n",
        "too_simple_model_predictions = np.mean(y) * np.ones_like(y)\n",
        "\n",
        "# В меру сложная модель\n",
        "X_ok = np.hstack([X, X**2, X**3])\n",
        "ok_model = LinearRegression()\n",
        "ok_model.fit(X_ok, y)\n",
        "ok_model_predictions = ok_model.predict(X_ok)\n",
        "\n",
        "# Очень сложная модель\n",
        "X_compl = np.hstack(\n",
        "    [X, X**2, X**3, X**4, X**5, X**6, X**7, X**8, X**9, X**10]\n",
        ")\n",
        "compl_model = LinearRegression()\n",
        "compl_model.fit(X_compl, y)\n",
        "compl_model_predictions = compl_model.predict(X_compl)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "plt.title(\"Сравнение моделей разной сложности\")\n",
        "plt.scatter(X, y, label=\"Тренировочная выборка\")\n",
        "plt.scatter(X_test, y_test, c=\"r\", label=\"Тестовая выборка\")\n",
        "plt.plot(X, too_simple_model_predictions, label=\"Очень простая модель\")\n",
        "plt.plot(X, ok_model_predictions, label=\"В меру сложная модель\")\n",
        "plt.plot(X, compl_model_predictions, label=\"Очень сложная модель\")\n",
        "plt.grid()\n",
        "plt.legend();"
      ],
      "id": "BTXQwUbTNpuB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsxCSZIINpuB"
      },
      "source": [
        "**Задание:** пользуясь определениями выше, прокоментируйте поведение моделей.\n",
        "\n",
        "**Задание:** оцените качественно величину смещения и дисперсии для следующих моделей:\n",
        "\n",
        "- Линейная регрессия, обучаемая на большой выборке без выбросов и линейно зависимых признаков, в которой признаки сильно коррелируют с целевой переменной.\n",
        "- Решающее дерево, которое строится до тех пор, пока в листах не окажется по одному объекту.\n",
        "- Логистическая регрессия, относящая все точки к одному классу."
      ],
      "id": "EsxCSZIINpuB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8ccaYJ1NpuC"
      },
      "source": [
        "### Bias-Variance tradeoff\n",
        "\n",
        "Из описания выше можно заметить, что при обучении моделей возникает выбор между смещением и дисперсией: недообученная модель имеет низкую дисперсию, но высокое смещение, а переобученная — низкое смещение, но высокую дисперсию. Этот выбор можно отобразить на картинке ([источник](https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update)). Здесь Total Error – ошибка на тестовой выборке (generalization error).\n",
        "\n",
        "![](https://www.bradyneal.com/img/bias-variance/fortmann-roe-bias-variance.png)\n",
        "\n",
        "Вывод из неё очевиден: строить следует оптимальные по сложности модели."
      ],
      "id": "L8ccaYJ1NpuC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnAc8JLXNpuC"
      },
      "source": [
        "## 2. От деревьев к случайному лесу\n",
        "\n",
        "### 2.1. Решающее дерево\n",
        "\n",
        "Мотивацию построения алгоритма случайного леса (Random Forest) удобно рассматривать в терминах смещения и дисперсии. Начнём с построения решающего дерева."
      ],
      "id": "DnAc8JLXNpuC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:49:31.346252Z",
          "start_time": "2023-05-10T11:49:31.338141Z"
        },
        "id": "so1KXDn9NpuC"
      },
      "outputs": [],
      "source": [
        "california = fetch_california_housing()\n",
        "california_X = pd.DataFrame(data=california.data, columns=california.feature_names)\n",
        "california_Y = california.target\n",
        "print(f\"X shape: {california_X.shape}, Y shape: {california_Y.shape}\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    california_X, california_Y, test_size=0.3, random_state=123, shuffle=True\n",
        ")"
      ],
      "id": "so1KXDn9NpuC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:49:31.459233Z",
          "start_time": "2023-05-10T11:49:31.346360Z"
        },
        "id": "clFPpPVkNpuD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# TODO: обучите решающее дерево без ограничений на тренировочной выборке\n",
        "\n",
        "# TODO: рассчитайте MSE на тренировочной и тестовой выборках"
      ],
      "id": "clFPpPVkNpuD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:49:47.314276Z",
          "start_time": "2023-05-10T11:49:31.461093Z"
        },
        "id": "MhyLGzvdNpuD"
      },
      "outputs": [],
      "source": [
        "from mlxtend.evaluate import bias_variance_decomp\n",
        "\n",
        "# TODO: выведите среднее смещение и среднюю дисперсию модели на тестовой выборке"
      ],
      "id": "MhyLGzvdNpuD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODOvXWTINpuD"
      },
      "source": [
        "Как мы обсуждали на предыдущем семинаре, такое дерево окажется сильно переобученным (высокая дисперсия и низкое смещение). Постараемся исправить это. На лекции мы обсуждали, что один из способов борьбы с переобучением – построение композиций моделей. На этом семинаре мы рассмотрим построение композиций при помощи бэггинга."
      ],
      "id": "ODOvXWTINpuD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjopczDwNpuD"
      },
      "source": [
        "### 2.2. Бэггинг\n",
        "\n",
        "Вспомним суть алгоритма:\n",
        "1. Обучаем много деревьев на бутстрапированных подвыборках исходной выборки независимо друг от друга. Бутстрапированную подвыборку строим при помощи выбора $N$ (размер исходной выборки) наблюдений из исходной выборки с возвращением.\n",
        "2. Усредняем предсказания всех моделей (например, берём арифметическое среднее).\n",
        "\n",
        "Можно показать, что модель, построенная при помощи бэггинга, будет иметь **то же смещение**, что и у отдельных деревьев, но значительно **меньшую дисперсию** (при выполнении некоторых условий)."
      ],
      "id": "RjopczDwNpuD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:50:53.873857Z",
          "start_time": "2023-05-10T11:49:47.316082Z"
        },
        "id": "EqNhFlLpNpuE"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "base_tree = DecisionTreeRegressor(random_state=123)\n",
        "\n",
        "# TODO: обучите бэггинг с 20 деревьями, каждое из которых строится без ограничений\n",
        "\n",
        "# TODO: рассчитайте MSE на тренировочной и тестовой выборках\n",
        "\n",
        "# TODO: выведите среднее смещение и среднюю дисперсию модели на тестовой выборке"
      ],
      "id": "EqNhFlLpNpuE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNHQw3VdNpuE"
      },
      "source": [
        "Как мы видим, по сравнению с единичным деревом смещение практически не изменилось, но дисперсия уменьшилась в несколько раз! Среднеквадратичная ошибка на тренировочной выборке больше не равна 0, а на тестовой — уменьшилась, что говорит о том, что мы успешно победили переобучение единичного решающего дерева.\n",
        "\n",
        "Можем ли мы снизить переобучение ещё сильнее? Можем!"
      ],
      "id": "aNHQw3VdNpuE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmx-vYenNpuE"
      },
      "source": [
        "### 2.3. Случайный лес\n",
        "\n",
        "При построении каждого дерева в бэггинге в ходе создания очередного узла будем выбирать случайный набор признаков, на основе которых производится разбиение. В результате такой процедуры мы уменьшим корреляцию между деревьями, за счёт чего снизим дисперсию итоговой модели. Такой алгоритм назвывается **случайным лесом** (Random Forest).\n",
        "\n",
        "По сравнению с единичным деревом к параметрам случайного леса добавляются:\n",
        "- `max_features` – число признаков, на основе которых проводятся разбиения при построении дерева.\n",
        "\n",
        "- `n_estimators` – число деревьев.\n",
        "\n",
        "Естественно, все параметры, относящиеся к единичному дереву, сохраняются для случайного леса."
      ],
      "id": "Dmx-vYenNpuE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:51:54.113083Z",
          "start_time": "2023-05-10T11:50:53.876014Z"
        },
        "id": "S9N_98wQNpuE"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# TODO: обучите случайный лес с 20 деревьями, каждое из которых строится без ограничений\n",
        "\n",
        "# TODO: рассчитайте MSE на тренировочной и тестовой выборках\n",
        "\n",
        "# TODO: выведите среднее смещение и среднюю дисперсию модели на тестовой выборке"
      ],
      "id": "S9N_98wQNpuE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E94fLFBeNpuE"
      },
      "source": [
        "Как мы видим, по сравнению с бэггингом смещение вновь осталось практически неизменным, а дисперсия немного уменьшилась. Конечно, если подобрать хорошие гиперпараметры, то получится снизить дисперсию ещё больше.\n",
        "\n",
        "Ошибка на тренировочной выборке увеличилась, а на тестовой — уменьшилась, что означает, что мы добились нашей цели в борьбе с переобученными деревьями!"
      ],
      "id": "E94fLFBeNpuE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndM4i2l2NpuE"
      },
      "source": [
        "## 3. Особенности случайного леса\n",
        "\n",
        "### 3.1. Число деревьев и \"Случайный лес не переобучается\"\n",
        "\n",
        "В своём [блоге](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#remarks) Лео Бриман (Leo Breiman), создатель случайного леса, написал следующее:\n",
        "\n",
        "> Random forest does not overfit. You can run as many trees as you want.\n",
        "\n",
        "**Обратите внимание:** как говорилось на лекции, случайный лес не переобучается именно с ростом числа деревьев (за счёт совместной работы бэггинга и использования случайных подпространств), но не в принципе. Посмотрим на поведение случайного леса при росте числа деревьев."
      ],
      "id": "ndM4i2l2NpuE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:52:35.735267Z",
          "start_time": "2023-05-10T11:51:54.114270Z"
        },
        "id": "9CdXyHdkNpuE"
      },
      "outputs": [],
      "source": [
        "n_trees = 100\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for i in trange(1, n_trees, 2):\n",
        "    rf = RandomForestRegressor(n_estimators=i, random_state=123, n_jobs=4)\n",
        "    rf.fit(X_train, y_train)\n",
        "    train_loss.append(mean_squared_error(y_train, rf.predict(X_train)))\n",
        "    test_loss.append(mean_squared_error(y_test, rf.predict(X_test)))\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"Изменение лосса в зависимости от количества деревьев\")\n",
        "plt.grid()\n",
        "plt.plot(train_loss, label=\"MSE_train\")\n",
        "plt.plot(test_loss, label=\"MSE_test\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.xlabel(\"# trees\")\n",
        "plt.legend();"
      ],
      "id": "9CdXyHdkNpuE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XltPFVH-NpuF"
      },
      "source": [
        "Как и ожидалось, по достижении некоторого числа деревьев обе ошибки практически не изменяются, то есть переобучения при росте числа деревьев не происходит. Однако практика показывает, что при изменении какого-нибудь другого параметра на реальных данных переобучение может произойти: [пример 1](https://datascience.stackexchange.com/questions/1028/do-random-forest-overfit), [пример 2](https://mljar.com/blog/random-forest-overfitting/). Например, случайный лес с ограниченными по глубине деревьями может предсказывать более точно, чем лес без ограничений. В нашем же случае случайный лес, скорее, лишь страдает от регуляризации. Например, посмотрим на поведение модели при изменении максимальной глубины деревьев (поэксперементируйте с другими параметрами)."
      ],
      "id": "XltPFVH-NpuF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:52:43.149372Z",
          "start_time": "2023-05-10T11:52:35.737047Z"
        },
        "id": "xAubpngBNpuF"
      },
      "outputs": [],
      "source": [
        "max_depth = 50\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for i in trange(1, max_depth, 2):\n",
        "    rf = RandomForestRegressor(n_estimators=20, max_depth=i, random_state=123, n_jobs=4)\n",
        "    rf.fit(X_train, y_train)\n",
        "    train_loss.append(mean_squared_error(y_train, rf.predict(X_train)))\n",
        "    test_loss.append(mean_squared_error(y_test, rf.predict(X_test)))\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"Изменение лосса в зависимости от глубины деревьев\")\n",
        "plt.grid()\n",
        "plt.plot(train_loss, label=\"MSE_train\")\n",
        "plt.plot(test_loss, label=\"MSE_test\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.xlabel(\"max_depth\")\n",
        "plt.legend();"
      ],
      "id": "xAubpngBNpuF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm99Bau8NpuF"
      },
      "source": [
        "Переобучение не наблюдается. Вообще же, как обычно, гиперпараметры случайного леса стоит подбирать на кросс-валидации."
      ],
      "id": "Cm99Bau8NpuF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd1e_R7zNpuF"
      },
      "source": [
        "### 3.2. Out-of-bag-ошибка\n",
        "\n",
        "Как мы обсудили выше, при построении случайного леса каждое дерево строится на бутстрапированной подвыборке, полученной из исходной обучающей выборки случайным набором с повторениями. Понятно, что некоторые наблюдения попадут в такую подвыборку несколько раз, а некоторые не войдут в неё вообще. Для каждого дерева мы можем рассмотреть объекты, которые не участвовали в обучении и использовать их для валидации.\n",
        "\n",
        "Усреднённая ошибка на неотобранных образцах по всему случайному лесу называется **out-of-bag-ошибкой**."
      ],
      "id": "Rd1e_R7zNpuF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:52:44.771132Z",
          "start_time": "2023-05-10T11:52:43.162809Z"
        },
        "id": "wPHGz978NpuF"
      },
      "outputs": [],
      "source": [
        "# oob_score_ = R2 на невиденных наблюдениях\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=123, oob_score=True, n_jobs=4)\n",
        "rf.fit(X_train, y_train)\n",
        "rf.oob_score_"
      ],
      "id": "wPHGz978NpuF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERfMarhpNpuF"
      },
      "source": [
        "### 3.3. Важность признаков\n",
        "Как и решающие деревья, случайный лес позволяет оценивать важность признаков. Важность признаков можно оценивать, например, по тому, насколько сильно уменьшается хаотичность при применении предикатов на основе этого признака."
      ],
      "id": "ERfMarhpNpuF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:52:44.862097Z",
          "start_time": "2023-05-10T11:52:44.778753Z"
        },
        "id": "GPVvBbzcNpuF"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(california[\"feature_names\"], rf.feature_importances_);"
      ],
      "id": "GPVvBbzcNpuF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXLRTYS7NpuF"
      },
      "source": [
        "Будьте осторожны с сильно коррелирующими признаками. Посмотрим, что произойдёт с важностью, если добавить в выборку линейно зависимый признак."
      ],
      "id": "nXLRTYS7NpuF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:52:44.865208Z",
          "start_time": "2023-05-10T11:52:44.863222Z"
        },
        "id": "jKP25Q3INpuG"
      },
      "outputs": [],
      "source": [
        "RM_mc = np.array((X_train.iloc[:, 5] * 2 + 3)).reshape(-1, 1)\n",
        "X_train_new = np.hstack((X_train, RM_mc))"
      ],
      "id": "jKP25Q3INpuG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:52:46.662664Z",
          "start_time": "2023-05-10T11:52:44.866029Z"
        },
        "id": "vljOpYGENpuG"
      },
      "outputs": [],
      "source": [
        "rf.fit(X_train_new, y_train)"
      ],
      "id": "vljOpYGENpuG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:52:46.743774Z",
          "start_time": "2023-05-10T11:52:46.664943Z"
        },
        "id": "IAUMu0SGNpuG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "names = list(california[\"feature_names\"])\n",
        "names.append(\"_AveOccup\")\n",
        "plt.bar(names, rf.feature_importances_);"
      ],
      "id": "IAUMu0SGNpuG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhpBT698NpuG"
      },
      "source": [
        "Важности перераспределились между линейной зависимыми признаками `AveOccup` и `_AveOccup`. Не забывайте учитывать корреляции между признаками, если вы используете этот метод для отбора признаков. Также обратите внимание на предупреждение в документации `sklearn`: не стоит использовать этот метод и для признаков, в которых есть много уникальных значений (например, категориальные признаки с небольшим числом категорий)."
      ],
      "id": "hhpBT698NpuG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaTEDTzsNpuG"
      },
      "source": [
        "## 4. Бонусная часть. Тестирование случайного леса на разных данных\n",
        "Ниже представлены шаблоны для сравнения случайного леса и других моделей на данных разных типов. Проведите побольше экспериментов, используя разные модели и метрики. Попробуйте подобрать гиперпараметры случайного леса так, чтобы достичь какого-нибудь порога качества.\n",
        "\n",
        "**NB: Случайный лес может обучаться достаточно долго.**\n",
        "\n",
        "### 4.1. Бинарная классификация на примере [Kaggle Predicting a Biological Response](https://www.kaggle.com/c/bioresponse/data?select=train.csv)"
      ],
      "id": "vaTEDTzsNpuG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:52:46.746170Z",
          "start_time": "2023-05-10T11:52:46.744028Z"
        },
        "id": "zJLawUzENpuG"
      },
      "outputs": [],
      "source": [
        "# Загрузка данных\n",
        "# !wget  -O 'kaggle_response.csv' -q 'https://www.dropbox.com/s/uha70sej5ugcrur/_train_sem09.csv?dl=1'"
      ],
      "id": "zJLawUzENpuG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:52:47.043457Z",
          "start_time": "2023-05-10T11:52:46.747687Z"
        },
        "id": "U96KGhLmNpuG"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"kaggle_response.csv\")\n",
        "X = data.iloc[:, 1:].values\n",
        "y = data.iloc[:, 0].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=123\n",
        ")"
      ],
      "id": "U96KGhLmNpuG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:52:47.915340Z",
          "start_time": "2023-05-10T11:52:47.044586Z"
        },
        "id": "yW_a2cZFNpuH"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# TODO: обучите логистическую регрессию и случайный лес с дефолтными параметрами\n",
        "# Сравните их AUC ROC на тестовой выборке"
      ],
      "id": "yW_a2cZFNpuH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ee9KUqiNpuH"
      },
      "source": [
        "### 4.2. Изображения на примере [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)"
      ],
      "id": "4ee9KUqiNpuH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:52:48.522914Z",
          "start_time": "2023-05-10T11:52:47.916081Z"
        },
        "id": "j_2A-1dsNpuH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "id": "j_2A-1dsNpuH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:52:48.583920Z",
          "start_time": "2023-05-10T11:52:48.531764Z"
        },
        "id": "S_kKMK2xNpuH"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X_train[0, :]);"
      ],
      "id": "S_kKMK2xNpuH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-10T11:53:32.049629Z",
          "start_time": "2023-05-10T11:52:48.601595Z"
        },
        "id": "arCmjOFtNpuH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# TODO: обучите случайный лес и kNN с дефолтными параметрами\n",
        "# Сравните их доли правильных ответов на тестовой выборке"
      ],
      "id": "arCmjOFtNpuH"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}